{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752fff78-7629-427f-a4e0-2d04427500e1",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b598866-d2b3-449a-b67b-0350e5985ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\1090135\\appdata\\local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\1090135\\appdata\\local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\1090135\\appdata\\local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8406480-1a8f-4175-9401-09fa78c27cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import numpy as np\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import faiss\n",
    "from llama_index.llms.cohere import Cohere\n",
    "import cohere\n",
    "from cohere import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa55221d-e6fa-4f6c-ab8d-dbbb36c6e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1a0fcc-786f-4541-9ca2-1b3d6e53f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2f6f7-a5cc-4284-bba5-10b2ceccd0c3",
   "metadata": {},
   "source": [
    "# Document handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f6df3-69e4-44f6-9a94-e6be347c4eaf",
   "metadata": {},
   "source": [
    "### doc loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58531281-a016-486a-ac3d-ddec0ec719c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc_loader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_text(self):\n",
    "        with open(self.file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d402-c2aa-43c3-aec3-4b31d10efa9a",
   "metadata": {},
   "source": [
    "### doc splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e841478c-2b7d-4ae1-aa3a-304a25ceddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc_splitter:\n",
    "    def __init__(self, chunk_size=512, chunk_overlap= 16):\n",
    "        self.splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, paragraph_separator=\"/n/n/n/n\")\n",
    "\n",
    "    def split(self, text):\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7697-864e-49a9-95d5-d193ad72d6c0",
   "metadata": {},
   "source": [
    "# Embedding Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4778db0-2940-4fdf-9f87-412f96d25937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_model_class:\n",
    "    def __init__(self, embed_model_name):\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "    def get_embedding(self, chunks):\n",
    "        embedding = [self.embed_model.get_text_embedding(chunk) for chunk in chunks]\n",
    "        return np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f687b7-5143-4cdc-9be9-ab668448a94b",
   "metadata": {},
   "source": [
    "# FAISS Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c965f0dc-9aca-4bc7-8628-a61086582a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faiss_indexing:\n",
    "    def __init__(self, dimension):\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "    def indexing(self, embeddings):\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        if embeddings.shape[1] != self.index.d:\n",
    "            raise ValueError(f\"Embedding dimension mismatch: Expected {self.index.d}, got {embeddings.shape[1]}\")\n",
    "        self.index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de692b08-dab2-4116-adf8-e767cd4882f5",
   "metadata": {},
   "source": [
    "# Creating Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6a9eda-f884-446e-b34b-f75e473a5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = CO_API_KEY\n",
    "cohere_client = Client(api_key)\n",
    "\n",
    "class Query_engine:\n",
    "    def __init__(self, llm_model=\"command-r-plus\", apikey=CO_API_KEY):\n",
    "        # Initialize the Cohere client with the provided API key\n",
    "        self.llm = cohere.Client(apikey)\n",
    "        self.model = llm_model\n",
    "\n",
    "    def query(self, search_content, query):\n",
    "        \"\"\"\n",
    "        Takes the searched similar content (text chunk) and the original query,\n",
    "        and processes them using the LLM to generate a coherent response.\n",
    "        \"\"\"\n",
    "        context = \"\\n\".join(search_content)  # Join search content into a single string for context\n",
    "\n",
    "        try:\n",
    "            # Query the LLM model with the provided context and query\n",
    "            response = self.llm.generate(\n",
    "                model=self.model,  # Use the provided model\n",
    "                prompt=f\"Query: {query}\\nContext: {context}\",\n",
    "                max_tokens=150,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Return the LLM response text\n",
    "            return response.generations[0].text  # Adjust this to match the response structure of the `cohere` client\n",
    "\n",
    "        except Exception as e:\n",
    "            # If an error occurs, print it and return a friendly message\n",
    "            print(f\"Error querying LLM: {e}\")\n",
    "            return \"Sorry, there was an error processing your request.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f346ff9-e1e6-402b-bbb4-44835095e923",
   "metadata": {},
   "source": [
    "# Vector Store Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057001a2-3179-4bed-96e2-4c760092983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissVectorStore:\n",
    "    def __init__(self, file_path, embed_model, chunk_size=512, chunk_overlap=16):\n",
    "        # Instead of creating new instances of doc_loader, doc_splitter, embed_model_class inside the class,\n",
    "        # we pass the instances to the constructor.\n",
    "        self.document_loader = doc_loader(file_path)\n",
    "        self.document_splitter = doc_splitter(chunk_size, chunk_overlap)\n",
    "        self.embedding_model = embed_model\n",
    "        self.index = None\n",
    "\n",
    "    def build_index(self):\n",
    "        # Use the passed instances directly\n",
    "        text = self.document_loader.load_text()\n",
    "        self.chunks = self.document_splitter.split(text)\n",
    "\n",
    "        # Generate embeddings for the chunks using the passed embed_model instance\n",
    "        embeddings = self.embedding_model.get_embedding(self.chunks)\n",
    "\n",
    "        # Create a FAISS index with the appropriate dimension (based on embedding size)\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        \n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        # Convert the query into an embedding using the passed embed_model instance\n",
    "        query_embedding = self.embedding_model.get_embedding([query])\n",
    "\n",
    "        # Perform a similarity search using the FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return distances, indices\n",
    "\n",
    "\n",
    "    def get_text_by_index(self, idx):\n",
    "        \"\"\"Retrieve the text chunk based on the index.\"\"\"\n",
    "        # Ensure the index is valid and return the corresponding chunk\n",
    "        if idx < len(self.chunks):\n",
    "            return self.chunks[idx]\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33708bf9-4de7-4ed7-a753-5e82d22d1b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.8233491  0.85043573 0.8664963  0.8756935  0.87710845]]\n",
      "Indices: [[147 144  64 152 151]]\n",
      "\n",
      "\n",
      "Text for index 147: _John Keats._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRAISE THE GENEROUS GODS FOR GIVING\n",
      "\n",
      "\n",
      "Some of us find joy in toil, some in art, some in the open air and the\n",
      "sunshine. All of us find it in simply being alive. Life is the gift no\n",
      "creature in his right mind would part with. As Milton asks,\n",
      "\n",
      "  \"For who would lose,\n",
      "  Though full of pain, this intellectual being,\n",
      "  These thoughts that wander through eternity,\n",
      "  To perish rather, swallowed up and lost\n",
      "  In the wide womb of uncreated night,\n",
      "  Devoid of sense and motion?\"\n",
      "\n",
      "\n",
      "  Praise the generous gods for giving\n",
      "    In a world of wrath and strife,\n",
      "  With a little time for living,\n",
      "    Unto all the joy of life.\n",
      "\n",
      "  At whatever source we drink it,\n",
      "    Art or love or faith or wine,\n",
      "  In whatever terms we think it,\n",
      "    It is common and divine.\n",
      "\n",
      "  Praise the high gods, for in giving\n",
      "    This to man, and this alone,\n",
      "  They have made his chance of living\n",
      "    Shine the equal of their own.\n",
      "\n",
      "\n",
      "_William Ernest Henley._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COWARDS\n",
      "\n",
      "\n",
      "We might as well accept the inevitable as the inevitable. There is no\n",
      "escaping death or taxes.\n",
      "\n",
      "\n",
      "  Cowards die many times before their deaths:\n",
      "  The valiant never taste of death but once.\n",
      "  Of all the wonders that I yet have heard,\n",
      "  It seems to me most strange that men should fear;\n",
      "  Seeing that death, a necessary end,\n",
      "  Will come, when it will come.\n",
      "\n",
      "\n",
      "_William Shakespeare._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE WORD\n",
      "\n",
      "\n",
      "The Cumaean sibyl offered Tarquin the Proud nine books for what seemed\n",
      "an exorbitant sum. He refused. She burned three of the books, and placed\n",
      "the same price on the six as on the original nine. Again he refused. She\n",
      "burned three more books, and offered the remainder for the sum she first\n",
      "named. This time Tarquin accepted. The books were found to contain\n",
      "prophecies and invaluable directions regarding Roman policy, but alas,\n",
      "they were no longer complete. So it is with joy. To take it now is to\n",
      "get it in its entirety. To defer until some other occasion is to get\n",
      "less of it--at the same cost.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the components outside of FaissVectorStore\n",
    "file_path = \"data/pg10763.txt\"\n",
    "embed_model = embed_model_class(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Pass the created instances to the FaissVectorStore constructor\n",
    "vector_store = FaissVectorStore(file_path=file_path, embed_model=embed_model)\n",
    "\n",
    "# Build the index (this will load, split, embed, and index the document)\n",
    "vector_store.build_index()\n",
    "\n",
    "# Example query for searching\n",
    "query = \"What do the Sikh Stoics believe?\"\n",
    "\n",
    "# Perform similarity search for the query\n",
    "distances, indices = vector_store.search(query, k=5)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Distances: {distances}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "\n",
    "\n",
    "index_to_retrieve = 147\n",
    "chunk_text = vector_store.get_text_by_index(index_to_retrieve)\n",
    "\n",
    "# Print the chunk text\n",
    "print(f\"\\n\\nText for index {index_to_retrieve}: {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd265976-abac-4107-9d8b-fdadb6c79a6d",
   "metadata": {},
   "source": [
    "# responese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b501df-210c-4cdc-99f7-12df2bd0ef9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sikhs believe in one God and that the purpose of life is to learn to love God.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_class = Query_engine()\n",
    "response = response_class.query(chunk_text, query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cedc50-d11d-4dd1-a4c6-8700d9291bbb",
   "metadata": {},
   "source": [
    "# Running full Agian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc47cd7d-9685-4af0-b517-b9924e8bb959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"If\" and \"When Earth\\'s Last Picture Is Painted\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the components outside of FaissVectorStore\n",
    "file_path = \"data/pg10763.txt\"\n",
    "embed_model = embed_model_class(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Pass the created instances to the FaissVectorStore constructor\n",
    "vector_store = FaissVectorStore(file_path=file_path, embed_model=embed_model)\n",
    "\n",
    "# Build the index (this will load, split, embed, and index the document)\n",
    "vector_store.build_index()\n",
    "\n",
    "# Example query for searching\n",
    "query = \"What poems by Rudyard Kipling are in this book?\"\n",
    "\n",
    "# Perform similarity search for the query\n",
    "distances, indices = vector_store.search(query, k=5)\n",
    "\n",
    "indices_flatterend = indices.flatten() if len(indices.shape) >1 else indices\n",
    "\n",
    "context = []\n",
    "\n",
    "for idx in  indices_flatterend:\n",
    "    try:\n",
    "        chunk_text = vector_store.get_text_by_index(idx)\n",
    "        context.append(chunk_text)\n",
    "    except IndexError:\n",
    "        print(f\"Error: Index {idx} out of range.\")\n",
    "\n",
    "response_class = Query_engine()\n",
    "response = response_class.query(context, query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2860f5-5baa-4492-9b0c-c0f595eb4bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc66b11-7cd6-4674-9ecd-25eba4bd2b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex (LinkedIn Learning)",
   "language": "python",
   "name": "lil_llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
