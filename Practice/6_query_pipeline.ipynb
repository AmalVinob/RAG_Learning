{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752fff78-7629-427f-a4e0-2d04427500e1",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8406480-1a8f-4175-9401-09fa78c27cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import numpy as np\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import faiss\n",
    "from llama_index.llms.cohere import Cohere\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa55221d-e6fa-4f6c-ab8d-dbbb36c6e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1a0fcc-786f-4541-9ca2-1b3d6e53f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519f85b-e8c2-41ad-b0e4-bf84379d4515",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b218fbb-94e6-4075-b9d9-331bfae54c6b",
   "metadata": {},
   "source": [
    "# query_pipeline example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2f6f7-a5cc-4284-bba5-10b2ceccd0c3",
   "metadata": {},
   "source": [
    "# Document handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f6df3-69e4-44f6-9a94-e6be347c4eaf",
   "metadata": {},
   "source": [
    "### doc loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58531281-a016-486a-ac3d-ddec0ec719c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc_loader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_text(self):\n",
    "        with open(self.file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d402-c2aa-43c3-aec3-4b31d10efa9a",
   "metadata": {},
   "source": [
    "### doc splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e841478c-2b7d-4ae1-aa3a-304a25ceddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc_splitter:\n",
    "    def __init__(self, chunk_size=512, chunk_overlap= 16):\n",
    "        self.splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, paragraph_separator=\"/n/n/n/n\")\n",
    "\n",
    "    def split(self, text):\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7697-864e-49a9-95d5-d193ad72d6c0",
   "metadata": {},
   "source": [
    "# Embedding Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4778db0-2940-4fdf-9f87-412f96d25937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_model_class:\n",
    "    def __init__(self, embed_model_name):\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "    def get_embedding(self, chunks):\n",
    "        embedding = [self.embed_model.get_text_embedding(chunk) for chunk in chunks]\n",
    "        return np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f687b7-5143-4cdc-9be9-ab668448a94b",
   "metadata": {},
   "source": [
    "# FAISS Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c965f0dc-9aca-4bc7-8628-a61086582a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faiss_indexing:\n",
    "    def __init__(self, dimension):\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "    def indexing(self, embeddings):\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        if embeddings.shape[1] != self.index.d:\n",
    "            raise ValueError(f\"Embedding dimension mismatch: Expected {self.index.d}, got {embeddings.shape[1]}\")\n",
    "        self.index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de692b08-dab2-4116-adf8-e767cd4882f5",
   "metadata": {},
   "source": [
    "# Creating Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6a9eda-f884-446e-b34b-f75e473a5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = CO_API_KEY\n",
    "cohere_client = Client(api_key)\n",
    "\n",
    "class Query_engine:\n",
    "    def __init__(self, llm_model=\"command-r-plus\", apikey=CO_API_KEY):\n",
    "        # Initialize the Cohere client with the provided API key\n",
    "        self.llm = cohere.Client(apikey)\n",
    "        self.model = llm_model\n",
    "\n",
    "    def query(self, search_content, query):\n",
    "        \"\"\"\n",
    "        Takes the searched similar content (text chunk) and the original query,\n",
    "        and processes them using the LLM to generate a coherent response.\n",
    "        \"\"\"\n",
    "        context = \"\\n\".join(search_content)  # Join search content into a single string for context\n",
    "\n",
    "        try:\n",
    "            # Query the LLM model with the provided context and query\n",
    "            response = self.llm.generate(\n",
    "                model=self.model,  # Use the provided model\n",
    "                prompt=f\"Query: {query}\\nContext: {context}\",\n",
    "                max_tokens=150,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Return the LLM response text\n",
    "            return response.generations[0].text  # Adjust this to match the response structure of the `cohere` client\n",
    "\n",
    "        except Exception as e:\n",
    "            # If an error occurs, print it and return a friendly message\n",
    "            print(f\"Error querying LLM: {e}\")\n",
    "            return \"Sorry, there was an error processing your request.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f346ff9-e1e6-402b-bbb4-44835095e923",
   "metadata": {},
   "source": [
    "# Vector Store Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057001a2-3179-4bed-96e2-4c760092983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissVectorStore:\n",
    "    def __init__(self, file_path, embed_model, chunk_size=512, chunk_overlap=16):\n",
    "        # Instead of creating new instances of doc_loader, doc_splitter, embed_model_class inside the class,\n",
    "        # we pass the instances to the constructor.\n",
    "        self.document_loader = doc_loader(file_path)\n",
    "        self.document_splitter = doc_splitter(chunk_size, chunk_overlap)\n",
    "        self.embedding_model = embed_model\n",
    "        self.index = None\n",
    "\n",
    "    def build_index(self):\n",
    "        # Use the passed instances directly\n",
    "        text = self.document_loader.load_text()\n",
    "        self.chunks = self.document_splitter.split(text)\n",
    "\n",
    "        # Generate embeddings for the chunks using the passed embed_model instance\n",
    "        embeddings = self.embedding_model.get_embedding(self.chunks)\n",
    "\n",
    "        # Create a FAISS index with the appropriate dimension (based on embedding size)\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = Faiss_indexing(dimension)\n",
    "        \n",
    "        # Add the embeddings to the FAISS index\n",
    "        self.index.indexing(embeddings)\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        # Convert the query into an embedding using the passed embed_model instance\n",
    "        query_embedding = self.embedding_model.get_embedding([query])\n",
    "\n",
    "        # Perform a similarity search using the FAISS index\n",
    "        distances, indices = self.index.index.search(query_embedding, k)\n",
    "        return distances, indices\n",
    "\n",
    "\n",
    "    def get_text_by_index(self, idx):\n",
    "        \"\"\"Retrieve the text chunk based on the index.\"\"\"\n",
    "        # Ensure the index is valid and return the corresponding chunk\n",
    "        if idx < len(self.chunks):\n",
    "            return self.chunks[idx]\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33708bf9-4de7-4ed7-a753-5e82d22d1b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.8233491  0.85043573 0.8664963  0.8756935  0.87710845]]\n",
      "Indices: [[147 144  64 152 151]]\n",
      "\n",
      "\n",
      "Text for index 147: _John Keats._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRAISE THE GENEROUS GODS FOR GIVING\n",
      "\n",
      "\n",
      "Some of us find joy in toil, some in art, some in the open air and the\n",
      "sunshine. All of us find it in simply being alive. Life is the gift no\n",
      "creature in his right mind would part with. As Milton asks,\n",
      "\n",
      "  \"For who would lose,\n",
      "  Though full of pain, this intellectual being,\n",
      "  These thoughts that wander through eternity,\n",
      "  To perish rather, swallowed up and lost\n",
      "  In the wide womb of uncreated night,\n",
      "  Devoid of sense and motion?\"\n",
      "\n",
      "\n",
      "  Praise the generous gods for giving\n",
      "    In a world of wrath and strife,\n",
      "  With a little time for living,\n",
      "    Unto all the joy of life.\n",
      "\n",
      "  At whatever source we drink it,\n",
      "    Art or love or faith or wine,\n",
      "  In whatever terms we think it,\n",
      "    It is common and divine.\n",
      "\n",
      "  Praise the high gods, for in giving\n",
      "    This to man, and this alone,\n",
      "  They have made his chance of living\n",
      "    Shine the equal of their own.\n",
      "\n",
      "\n",
      "_William Ernest Henley._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COWARDS\n",
      "\n",
      "\n",
      "We might as well accept the inevitable as the inevitable. There is no\n",
      "escaping death or taxes.\n",
      "\n",
      "\n",
      "  Cowards die many times before their deaths:\n",
      "  The valiant never taste of death but once.\n",
      "  Of all the wonders that I yet have heard,\n",
      "  It seems to me most strange that men should fear;\n",
      "  Seeing that death, a necessary end,\n",
      "  Will come, when it will come.\n",
      "\n",
      "\n",
      "_William Shakespeare._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE WORD\n",
      "\n",
      "\n",
      "The Cumaean sibyl offered Tarquin the Proud nine books for what seemed\n",
      "an exorbitant sum. He refused. She burned three of the books, and placed\n",
      "the same price on the six as on the original nine. Again he refused. She\n",
      "burned three more books, and offered the remainder for the sum she first\n",
      "named. This time Tarquin accepted. The books were found to contain\n",
      "prophecies and invaluable directions regarding Roman policy, but alas,\n",
      "they were no longer complete. So it is with joy. To take it now is to\n",
      "get it in its entirety. To defer until some other occasion is to get\n",
      "less of it--at the same cost.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the components outside of FaissVectorStore\n",
    "file_path = \"data/pg10763.txt\"\n",
    "embed_model = embed_model_class(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Pass the created instances to the FaissVectorStore constructor\n",
    "vector_store = FaissVectorStore(file_path=file_path, embed_model=embed_model)\n",
    "\n",
    "# Build the index (this will load, split, embed, and index the document)\n",
    "vector_store.build_index()\n",
    "\n",
    "# Example query for searching\n",
    "query = \"What do the Sikh Stoics believe?\"\n",
    "\n",
    "# Perform similarity search for the query\n",
    "distances, indices = vector_store.search(query, k=5)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Distances: {distances}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "\n",
    "\n",
    "index_to_retrieve = 147\n",
    "chunk_text = vector_store.get_text_by_index(index_to_retrieve)\n",
    "\n",
    "# Print the chunk text\n",
    "print(f\"\\n\\nText for index {index_to_retrieve}: {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cdf59-8611-450e-9631-73df0f57c89c",
   "metadata": {},
   "source": [
    "# Query_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55273819-f74c-46e0-9031-c8f7205ae992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e58886-78e9-47cb-9c08-a7ab55307afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d099-52ac-4204-8259-002988db2898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c025c5d-4ff3-438d-a02a-5627d0be5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_flatterend = indices.flatten() if len(indices.shape) >1 else indices\n",
    "\n",
    "context = []\n",
    "\n",
    "for idx in  indices_flatterend:\n",
    "    try:\n",
    "        chunk_text = vector_store.get_text_by_index(idx)\n",
    "        context.append(chunk_text)\n",
    "    except IndexError:\n",
    "        print(f\"Error: Index {idx} out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a1c440f-a949-4a17-8e6b-81c0883583b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_generate(prompt: str, max_tokens: int = 150, model: str = 'command-r-plus'):\n",
    "    \"\"\"Generate a response from Cohere's language model\"\"\"\n",
    "    try:\n",
    "        response = cohere_client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.text.strip()  # Extract the generated text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response with Cohere: {e}\")\n",
    "        return \"Sorry, something went wrong with the generation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c5b8dc4-ba77-40a7-876c-6d7065681861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import BaseRetriever\n",
    "import numpy as np\n",
    "\n",
    "class FaissRetriever():\n",
    "    def __init__(self, faiss_vector_store, k=5):\n",
    "        self.faiss_vector_store = faiss_vector_store\n",
    "        self.k = k  # Number of top results to retrieve\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        # Get the embeddings for the query\n",
    "        distances, indices = self.faiss_vector_store.search(query, k=self.k)\n",
    "        \n",
    "        # Retrieve the corresponding text chunks based on indices\n",
    "        context = []\n",
    "        indices_flat = indices.flatten() if len(indices.shape) > 1 else indices\n",
    "        \n",
    "        for idx in indices_flat:\n",
    "            try:\n",
    "                chunk_text = self.faiss_vector_store.get_text_by_index(idx)\n",
    "                context.append(chunk_text)\n",
    "            except IndexError:\n",
    "                print(f\"Error: Index {idx} out of range.\")\n",
    "        \n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a4af5dd-d77b-4291-b0b8-d870577cb57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Chain must be a sequence of modules or module keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m faiss_retriever \u001b[38;5;241m=\u001b[39m FaissRetriever(faiss_vector_store\u001b[38;5;241m=\u001b[39mvector_store, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# QueryPipeline using Cohere\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mQueryPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use FAISS-based retriever here\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcohere_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the Cohere LLM generation\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[0;32m     74\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do the Sikh Stoics believe?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:249\u001b[0m, in \u001b[0;36mQueryPipeline.__init__\u001b[1;34m(self, callback_manager, chain, modules, links, state, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    244\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([]),\n\u001b[0;32m    245\u001b[0m     state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    247\u001b[0m )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Pydantic validator isn't called for __init__ so we need to call it manually\u001b[39;00m\n\u001b[0;32m    251\u001b[0m get_and_update_stateful_components(\u001b[38;5;28mself\u001b[39m, state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:278\u001b[0m, in \u001b[0;36mQueryPipeline._init_graph\u001b[1;34m(self, chain, modules, links)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m links \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both chain and modules/links in init.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_modules(modules)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:302\u001b[0m, in \u001b[0;36mQueryPipeline.add_chain\u001b[1;34m(self, chain)\u001b[0m\n\u001b[0;32m    300\u001b[0m         module_keys\u001b[38;5;241m.\u001b[39mappend(module)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain must be a sequence of modules or module keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# then add all links\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chain) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Chain must be a sequence of modules or module keys."
     ]
    }
   ],
   "source": [
    "from cohere import Client\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Initialize the Cohere client with the provided API key\n",
    "class Query_engine:\n",
    "    def __init__(self, llm_model=\"command-r-plus\", apikey=\"CO_API_KEY\"):\n",
    "        self.llm = Client(apikey)\n",
    "        self.model = llm_model\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 150):\n",
    "        response = self.llm.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.text.strip()\n",
    "\n",
    "# Initialize your custom Query_engine (Cohere client)\n",
    "cohere_query_engine = Query_engine(apikey=\"CO_API_KEY\")\n",
    "\n",
    "# Create the prompt templates\n",
    "prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"Syntesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "Context:\n",
    "\n",
    "{query_str}\n",
    "\n",
    "Syntesized response:\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "# Custom FAISS Retriever (as defined before)\n",
    "class FaissRetriever():\n",
    "    def __init__(self, faiss_vector_store, k=5):\n",
    "        self.faiss_vector_store = faiss_vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        distances, indices = self.faiss_vector_store.search(query, k=self.k)\n",
    "        \n",
    "        context = []\n",
    "        indices_flat = indices.flatten() if len(indices.shape) > 1 else indices\n",
    "        \n",
    "        for idx in indices_flat:\n",
    "            try:\n",
    "                chunk_text = self.faiss_vector_store.get_text_by_index(idx)\n",
    "                context.append(chunk_text)\n",
    "            except IndexError:\n",
    "                print(f\"Error: Index {idx} out of range.\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Initialize the FAISS Retriever\n",
    "faiss_retriever = FaissRetriever(faiss_vector_store=vector_store, k=5)\n",
    "\n",
    "# QueryPipeline using Cohere\n",
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        prompt_tmpl1,\n",
    "        faiss_retriever,  # Use FAISS-based retriever here\n",
    "        prompt_tmpl2,\n",
    "        cohere_query_engine.generate,  # Use the Cohere LLM generation\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What do the Sikh Stoics believe?\"\n",
    "\n",
    "# Running the query\n",
    "response = p.query(query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2371fd-bee4-4d21-8900-f7f955955e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ce87ddb-b8a6-4f57-83a8-acfc3a0c43e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Chain must be a sequence of modules or module keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m faiss_retriever \u001b[38;5;241m=\u001b[39m FaissRetriever(faiss_vector_store\u001b[38;5;241m=\u001b[39mvector_store, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# QueryPipeline using Cohere\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mQueryPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use FAISS-based retriever here\u001b[39;49;00m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcohere_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the custom Cohere generator here\u001b[39;49;00m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[0;32m     85\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do the Sikh Stoics believe?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:249\u001b[0m, in \u001b[0;36mQueryPipeline.__init__\u001b[1;34m(self, callback_manager, chain, modules, links, state, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    244\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([]),\n\u001b[0;32m    245\u001b[0m     state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    247\u001b[0m )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Pydantic validator isn't called for __init__ so we need to call it manually\u001b[39;00m\n\u001b[0;32m    251\u001b[0m get_and_update_stateful_components(\u001b[38;5;28mself\u001b[39m, state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:278\u001b[0m, in \u001b[0;36mQueryPipeline._init_graph\u001b[1;34m(self, chain, modules, links)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m links \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both chain and modules/links in init.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_modules(modules)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:302\u001b[0m, in \u001b[0;36mQueryPipeline.add_chain\u001b[1;34m(self, chain)\u001b[0m\n\u001b[0;32m    300\u001b[0m         module_keys\u001b[38;5;241m.\u001b[39mappend(module)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain must be a sequence of modules or module keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# then add all links\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chain) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Chain must be a sequence of modules or module keys."
     ]
    }
   ],
   "source": [
    "from cohere import Client\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Initialize the Cohere client with the provided API key\n",
    "class Query_engine:\n",
    "    def __init__(self, llm_model=\"command-r-plus\", apikey=\"CO_API_KEY\"):\n",
    "        self.llm = Client(apikey)\n",
    "        self.model = llm_model\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 150):\n",
    "        response = self.llm.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.text.strip()\n",
    "\n",
    "# Custom Cohere Generator class for the QueryPipeline\n",
    "class CohereGenerator:\n",
    "    def __init__(self, query_engine: Query_engine):\n",
    "        self.query_engine = query_engine\n",
    "\n",
    "    def __call__(self, context: str) -> str:\n",
    "        # Use the Cohere query engine to generate a response from the context\n",
    "        prompt = f\"Context: {context}\\nSynthesize the answer using modern slang.\"\n",
    "        return self.query_engine.generate(prompt)\n",
    "\n",
    "# Initialize your custom Query_engine (Cohere client)\n",
    "cohere_query_engine = Query_engine(apikey=\"CO_API_KEY\")\n",
    "cohere_generator = CohereGenerator(query_engine=cohere_query_engine)\n",
    "\n",
    "# Create the prompt templates\n",
    "prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"Syntesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "Context:\n",
    "\n",
    "{query_str}\n",
    "\n",
    "Syntesized response:\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "# Custom FAISS Retriever (as defined before)\n",
    "class FaissRetriever():\n",
    "    def __init__(self, faiss_vector_store, k=5):\n",
    "        self.faiss_vector_store = faiss_vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        distances, indices = self.faiss_vector_store.search(query, k=self.k)\n",
    "        \n",
    "        context = []\n",
    "        indices_flat = indices.flatten() if len(indices.shape) > 1 else indices\n",
    "        \n",
    "        for idx in indices_flat:\n",
    "            try:\n",
    "                chunk_text = self.faiss_vector_store.get_text_by_index(idx)\n",
    "                context.append(chunk_text)\n",
    "            except IndexError:\n",
    "                print(f\"Error: Index {idx} out of range.\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Initialize the FAISS Retriever\n",
    "faiss_retriever = FaissRetriever(faiss_vector_store=vector_store, k=5)\n",
    "\n",
    "# QueryPipeline using Cohere\n",
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        prompt_tmpl1,\n",
    "        faiss_retriever,  # Use FAISS-based retriever here\n",
    "        prompt_tmpl2,\n",
    "        cohere_generator,  # Use the custom Cohere generator here\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What do the Sikh Stoics believe?\"\n",
    "\n",
    "# Running the query\n",
    "response = p.query(query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b719eaa7-083a-47dd-a992-8e7f27b1f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissRetriever():\n",
    "    def __init__(self, faiss_vector_store, k=5):\n",
    "        self.faiss_vector_store = faiss_vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        distances, indices = self.faiss_vector_store.search(query, k=self.k)\n",
    "        \n",
    "        context = []\n",
    "        indices_flat = indices.flatten() if len(indices.shape) > 1 else indices\n",
    "        \n",
    "        for idx in indices_flat:\n",
    "            try:\n",
    "                chunk_text = self.faiss_vector_store.get_text_by_index(idx)\n",
    "                context.append(chunk_text)\n",
    "            except IndexError:\n",
    "                print(f\"Error: Index {idx} out of range.\")\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "720eef5e-619b-4c42-8a03-79efb723f3dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Chain must be a sequence of modules or module keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m faiss_retriever \u001b[38;5;241m=\u001b[39m FaissRetriever(faiss_vector_store\u001b[38;5;241m=\u001b[39mvector_store, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Ensure that the `QueryPipeline` is properly initialized with callable modules\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mQueryPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# First step: prompt template to retrieve context\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Second step: FAISS retriever\u001b[39;49;00m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_tmpl2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Third step: prompt template to format the response\u001b[39;49;00m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcohere_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fourth step: use Cohere generator for synthesis\u001b[39;49;00m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[0;32m     85\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do the Sikh Stoics believe?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:249\u001b[0m, in \u001b[0;36mQueryPipeline.__init__\u001b[1;34m(self, callback_manager, chain, modules, links, state, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    244\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([]),\n\u001b[0;32m    245\u001b[0m     state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    247\u001b[0m )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Pydantic validator isn't called for __init__ so we need to call it manually\u001b[39;00m\n\u001b[0;32m    251\u001b[0m get_and_update_stateful_components(\u001b[38;5;28mself\u001b[39m, state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:278\u001b[0m, in \u001b[0;36mQueryPipeline._init_graph\u001b[1;34m(self, chain, modules, links)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m links \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both chain and modules/links in init.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_modules(modules)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\lil_llama_index\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:302\u001b[0m, in \u001b[0;36mQueryPipeline.add_chain\u001b[1;34m(self, chain)\u001b[0m\n\u001b[0;32m    300\u001b[0m         module_keys\u001b[38;5;241m.\u001b[39mappend(module)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain must be a sequence of modules or module keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# then add all links\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chain) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Chain must be a sequence of modules or module keys."
     ]
    }
   ],
   "source": [
    "from cohere import Client\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Initialize the Cohere client with the provided API key\n",
    "class Query_engine:\n",
    "    def __init__(self, llm_model=\"command-r-plus\", apikey=\"CO_API_KEY\"):\n",
    "        self.llm = Client(apikey)\n",
    "        self.model = llm_model\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 150):\n",
    "        response = self.llm.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.text.strip()\n",
    "\n",
    "# Custom Cohere Generator class for the QueryPipeline\n",
    "class CohereGenerator:\n",
    "    def __init__(self, query_engine: Query_engine):\n",
    "        self.query_engine = query_engine\n",
    "\n",
    "    def __call__(self, context: str) -> str:\n",
    "        # Use the Cohere query engine to generate a response from the context\n",
    "        prompt = f\"Context: {context}\\nSynthesize the answer using modern slang.\"\n",
    "        return self.query_engine.generate(prompt)\n",
    "\n",
    "# Initialize your custom Query_engine (Cohere client)\n",
    "cohere_query_engine = Query_engine(apikey=\"CO_API_KEY\")\n",
    "cohere_generator = CohereGenerator(query_engine=cohere_query_engine)\n",
    "\n",
    "# Create the prompt templates\n",
    "prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"Syntesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "Context:\n",
    "\n",
    "{query_str}\n",
    "\n",
    "Syntesized response:\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "# Custom FAISS Retriever (as defined before)\n",
    "class FaissRetriever():\n",
    "    def __init__(self, faiss_vector_store, k=5):\n",
    "        self.faiss_vector_store = faiss_vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        distances, indices = self.faiss_vector_store.search(query, k=self.k)\n",
    "        \n",
    "        context = []\n",
    "        indices_flat = indices.flatten() if len(indices.shape) > 1 else indices\n",
    "        \n",
    "        for idx in indices_flat:\n",
    "            try:\n",
    "                chunk_text = self.faiss_vector_store.get_text_by_index(idx)\n",
    "                context.append(chunk_text)\n",
    "            except IndexError:\n",
    "                print(f\"Error: Index {idx} out of range.\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Initialize the FAISS Retriever\n",
    "faiss_retriever = FaissRetriever(faiss_vector_store=vector_store, k=5)\n",
    "\n",
    "# Ensure that the `QueryPipeline` is properly initialized with callable modules\n",
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        prompt_tmpl1,  # First step: prompt template to retrieve context\n",
    "        faiss_retriever,  # Second step: FAISS retriever\n",
    "        prompt_tmpl2,  # Third step: prompt template to format the response\n",
    "        cohere_generator,  # Fourth step: use Cohere generator for synthesis\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What do the Sikh Stoics believe?\"\n",
    "\n",
    "# Running the query\n",
    "response = p.query(query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee8e1e-8d4f-48ad-b771-334070e7bd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe2b53a-b0d5-430e-9682-81b679219255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "# prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "# prompt_str2 = \"\"\"Syntesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "# Context:\n",
    "\n",
    "# {query_str}\n",
    "\n",
    "# Synthesized response:\n",
    "# \"\"\"\n",
    "# prompt_tmpl2 = PromptTemplate(prompt_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "409c75e2-b0a8-4f1c-8afd-2f12144cedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cohere_response_fn(query_str: str):\n",
    "#     \"\"\"Use Cohere to generate a response based on a query\"\"\"\n",
    "#     return cohere_generate(f\"Query: {query_str}\", max_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbdb48-e627-4399-858f-4512dfe9420b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe17671-a59a-4233-abbd-b3b17563b240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f25fb1-d0ee-4ee3-86ea-9cccffd16f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex (LinkedIn Learning)",
   "language": "python",
   "name": "lil_llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
